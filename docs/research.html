<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2024-11-05 Tue 08:49 -->
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Zi Liang (Research Page)</title>
<meta name="author" content="liangzid" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="/css/styles.css" /> <link rel="stylesheet" type="text/css" href="/css/htmlize.css" /> <script src="/scripts/script.js"></script> <script src="/scripts/toc.js"></script>
</head>
<body>
<div id="preamble" class="status">
<nav class="nav"> <a href="/index.html" class="button">Home</a> <a href="/sitemap.html" class="button">Sitemaps</a> </nav> <hr>
</div>
<div id="content" class="content">
<h1 class="title">Zi Liang (Research Page)</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org97412e6">1. Introduction</a>
<ul>
<li><a href="#org91311b7">1.1. Introducing Myself</a></li>
<li><a href="#org115af1f">1.2. Introducing My Recent Research</a>
<ul>
<li><a href="#org2762f87">1.2.1. Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models [Arxiv'24]</a></li>
<li><a href="#org96be7b5">1.2.2. MERGE: Fast Private Text Generation [AAAI'24]</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org0d931cc">2. Experiences</a></li>
<li><a href="#orgd1cc1b6">3. Publications</a></li>
<li><a href="#orgab4114f">4. Contact Me</a></li>
</ul>
</div>
</div>

<div id="org1c518d0" class="figure">
<p><img src="images/danjin.jpg" alt="danjin.jpg">
</p>
</div>

<div id="outline-container-org97412e6" class="outline-2">
<h2 id="org97412e6"><span class="section-number-2">1.</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org91311b7" class="outline-3">
<h3 id="org91311b7"><span class="section-number-3">1.1.</span> Introducing Myself</h3>
<div class="outline-text-3" id="text-1-1">
<p>
My name is Zi Liang, now a PhD student in the <a href="https://www.astaple.com/">Astaple Group</a> of Hong Kong Polytechnic University (PolyU). My supervisor is Prof. <a href="https://haibohu.org/">Haibo Hu</a>. I begin my research in Xi'an Jiaotong University, under the supervision of Prof. <a href="https://gr.xjtu.edu.cn/web/phwang">Pinghui Wang</a> and <a href="https://www.linkedin.com/in/ruofei">Ruofei (Bruce) Zhang</a>.
I also work closely with Dr. <a href="https://scholar.google.com.hk/citations?user=XzO2dV0AAAAJ&amp;hl=zh-CN">Nuo Xu</a>, <a href="https://scholar.google.com.hk/citations?user=Wd5IdkMAAAAJ&amp;hl=zh-TW">Shuo Zhang</a>, <a href="https://scholar.google.com/citations?user=spRkQ2oAAAAJ&amp;hl=en">Yaxin Xiao</a>, <a href="https://xinweizhang1998.github.io/xinweizhang.github.io/">Xinwei Zhang</a>, Dr. <a href="https://sites.google.com/view/liujunxu">Junxu Liu</a> and <a href="https://yywang.netlify.app/">Yanyun Wang</a>.
</p>


<p>
I focus on analyzing (pursue) the potential risks contained in current transformer-based large language models and dive into the analysis of why and how neural networks work and raise a vulnerability.
My research can be divided into the following two categories:
</p>

<ul class="org-ul">
<li><b>Revealing new threats or defence against existing attacks</b>. To provide a comprehensive evaluation on popular AI services or techniques incorporated with in-depth theoretical analysis or a series of intuitive explorations, such as my recent study in <i>prompt extraction attacks</i> (<a href="https://arxiv.org/abs/2408.02416">link</a>);</li>
<li><b>Better understanding to models and learning</b>. To explain why and how safety issues produced and what such a problem means during the training/inference of models.</li>
</ul>

<p>
Also, I am very familiar with <i>natural language processing</i> since 2020, especially in constructing conversational AIs. From 2024 I am also excited to the future of AI when employing <i>reinforcement learning</i>.
</p>
</div>
</div>

<div id="outline-container-org115af1f" class="outline-3">
<h3 id="org115af1f"><span class="section-number-3">1.2.</span> Introducing My Recent Research</h3>
<div class="outline-text-3" id="text-1-2">
</div>
<div id="outline-container-org2762f87" class="outline-4">
<h4 id="org2762f87"><span class="section-number-4">1.2.1.</span> Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models [Arxiv'24]</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
This paper uncover the threat of <b>prompt leakage</b> on customized prompt-based services, such as OpenAI's GPTs. It aims to answer three questions:
</p>
<ol class="org-ol">
<li>Can LLM's alignments defend against prompt extraction attacks?</li>
<li>How do LLMs leak their prompts?</li>
<li>Which factors of prompts and LLMs lead to such leakage?</li>
</ol>


<p>
We provide a comprehensive and systemic evaluation to answer question 1 and 3, and propose two hypothesis with experimental validation for question 2. We also propose several easy-to-adpot defending strategy based on our discovery.
</p>

<p>
Click <a href="https://arxiv.org/abs/2408.02416">here</a> if you are also interested in this research.
</p>
</div>
</div>

<div id="outline-container-org96be7b5" class="outline-4">
<h4 id="org96be7b5"><span class="section-number-4">1.2.2.</span> MERGE: Fast Private Text Generation [AAAI'24]</h4>
<div class="outline-text-4" id="text-1-2-2">
<p>
This paper propose a new privacy-preserving inference framework for current transformer-based generative language models based on Secret Sharing and Multi-party Security Computation (MPC). It is also the <b>first</b> private inference framework specifically designed for NLG models. 10x of speedup are provided via our propose method.
</p>

<p>
If you are curious about how cryptography protect the privacy of user contents and models and how we optimize the inference procedure, click <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29964">here</a> for more details.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org0d931cc" class="outline-2">
<h2 id="org0d931cc"><span class="section-number-2">2.</span> Experiences</h2>
<div class="outline-text-2" id="text-2">
<ol class="org-ol">
<li>2016.09-2020.06: Bachelor Degree, in Northeastern University, on <i>cybernetics (Control Theory)</i>;</li>
<li>2020.09-2023.06: Master Degree, in the iMiss Group of Xi'an Jiaotong University, on <i>software engineer</i> and research for <i>Conversational AI</i> and <i>NLP Security</i>;</li>
<li>2023.11-now: PhD Student, in the The Hong Kong Polytechnic University in Hong Kong. Research of interests: <i>AI security</i> and <i>Natural Language Processing</i>.</li>
</ol>
</div>
</div>
<div id="outline-container-orgd1cc1b6" class="outline-2">
<h2 id="orgd1cc1b6"><span class="section-number-2">3.</span> Publications</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li><i>Alignment-Aware Model Extraction Attacks on Large Language Models</i> Zi Liang, Qingqing Ye, Yanyun Wang, Sen Zhang, Yaxin Xiao, Ronghua Li, Jianliang Xu, Haibo Hu - arXiv preprint arXiv:2409.02718, 2024 [<a href="https://arxiv.org/abs/2409.02718">Paper</a>] [<a href="https://github.com/liangzid/LoRD-MEA">Code</a>]</li>
<li><i>PAIR: Pre-denosing Augmented Image Retrieval Model for Defending Adversarial Patches</i> Z Zhou, P Wang, <b>Z Liang</b>, R Zhang, H Bai - MM 2024</li>
<li><i>Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models</i> <b>Z Liang</b>, H Hu, Q Ye, Y Xiao, H Li - arXiv preprint arXiv:2408.02416, 2024 [<a href="https://arxiv.org/abs/2408.02416">Paper</a>][<a href="https://github.com/liangzid/PromptExtractionEval">Code</a>]</li>
<li><i>TSFool: Crafting Highly-Imperceptible Adversarial Time Series through Multi-Objective Attack</i> Yanyun Wang, Dehui Du, Haibo Hu,  <b>Zi Liang</b>, Yuanhao Liu - ECAI, 2024</li>
<li><i>Merge: Fast private text generation</i>  <b>Z Liang</b>, P Wang, R Zhang, N Xu, S Zhang, L Xing… - AAAI, 2024 [<a href="https://arxiv.org/abs/2305.15769">Paper</a>] [<a href="https://github.com/liangzid/MERGE">Code</a>]</li>
<li>"Healing Unsafe Dialogue Responses with Weak Supervision Signals." <b>Z Liang</b>, &#x2026; - arXiv preprint arXiv:2305.15757 (2023). [<a href="https://arxiv.org/abs/2305.15757">Paper</a>][<a href="https://github.com/liangzid/TEMP">Code</a>]</li>
<li><i>Multi-action dialog policy learning from logged user feedback</i> S Zhang, J Zhao, P Wang, T Wang,  <b>Z Liang</b>, J Tao… - AAAI, 2023</li>
</ul>
</div>
</div>
<div id="outline-container-orgab4114f" class="outline-2">
<h2 id="orgab4114f"><span class="section-number-2">4.</span> Contact Me</h2>
<div class="outline-text-2" id="text-4">
</div>
<ol class="org-ol">
<li><a id="orgc558c58"></a>GitHub: <a href="https://github.com/liangzid">https://github.com/liangzid</a><br></li>
<li><a id="orgc17f43f"></a>MAIL: zi1415926.liang@connect.polyu.hk<br></li>
<li><a id="orgfb58715"></a>Wechat: paperacceptplease<br></li>
<li><a id="org2480a6f"></a>Google Scholar: <a href="https://scholar.google.com/citations?user=pzrGwvMAAAAJ&amp;hl=zh-CN">HERE</a><br></li>
</ol>
</div>
</div>
<div id="postamble" class="status">
<hr class="Solid"> <div class="info"> <span class="author">Author: liangzid (<a href="mailto:2273067585@qq.com">2273067585@qq.com</a>)</span> <span class="date">Create Date: Fri Sep 10 14:23:46 2021</span> <span class="date">Last modified: 2024-11-05 Tue 08:47</span> <span>Creator: <a href="https://www.gnu.org/software/emacs/">Emacs</a> 29.2 (<a href="https://orgmode.org">Org</a> mode 9.6.28)</span> </div>
</div>
</body>
</html>
