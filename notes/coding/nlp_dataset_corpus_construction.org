#+title: pytorch-style的数据集构建
#+author: 梁子
#+mail: 2273067585@qq.com
#+date: <2021-03-28 周日>

#+BEGIN_QUOTE
这篇文字写给实习生.
#+END_QUOTE


训练集是进行深度学习训练的最重要环节。
现以NLP领域的例子展示做实验过程中涉及到对数据集进行处理的各种操作。
* pytorch定义数据集的基本样式
** 在此之前, 看一看数据集
数据集是什么?
数据集是这样的一种迭代器(简单理解,数据集就是一个列表), 这个迭代器里的每一个元素都是一个二元组(data, label). 比如,对于分类问题, 我们的数据就是图片,文本等等, 标签就是最终的分类结果, 
比如二分类问题,或许就是0和1.

所以,对于要做的实验,重点知道数据和标签是什么. 对于深度学习, 大概率 *数据就是输入,标签就是输出*.
** pytorch对此的封装
理解了数据集的基本格式,其实封装一个数据集就比较简单了. 需要在初始化的时候处理好数据, 然后在此基础上封装一个迭代器.
下面的代码是对一个基本例子的展示
#+BEGIN_SRC python
  class NLG_Normal_dataset(Dataset):
      """NLG input format for normal model"""
      def __init__(self, tokenizer, mode='train'):
	  self.max_sentence_length=tokenizer.get_max_seq_length()
	  self.tokenizer=tokenizer

	  self.path=os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
	  with open("./dicts_data_save_for_dataset.pk",'rb') as f:
	      traindict,testdict,tunedict=pickle.load(f)
	  self.dataset=[]
	  # 此处单拣训练集作为测试例子,其他地方从略.
	  for response in traindict.keys():
	      acts=traindict[response]
	      string_act=""
	      for ele_ls in acts:
		  per_seq=""
		  for ele in ele_ls:
		      per_seq+=ele
		  string_act+=per_seq+","
	      output1=self.tokenizer.single_convert_text_into_indextokens_and_segment_id(response)[0]
	      mask1=self.tokenizer.single_convert_text_into_indextokens_and_segment_id(response)[2]
	      input2=self.tokenizer.single_convert_text_into_indextokens_and_segment_id(string_act)[0]
	      mask2=self.tokenizer.single_convert_text_into_indextokens_and_segment_id(string_act)[2]
	      self.dataset.append(((input2,mask2),(output1,mask1)))

      def __getitem__(self,i):
	  return self.dataset[i]

      def __len__(self):
	  return len(self.dataset)
#+END_SRC

我们不要先去看具体的细节, 这个数据集在封装模式上需要包括以下三个特点:
1. 继承自Dataset. 实际上是继承自torch.utils.data.Dataset
2. 需要有方法__getitem__ . 这个方法输入一个必要的索引i, 然后去返回迭代器的第i个样本. 也就是数据集里的第i个样本.
3. 需要有方法__len__ . 这个方法输出整个迭代器的长度.

那么, 进行初始化的时候,主要是在干嘛? 其实就是在干一件事, 获得数据集样本列表. 也就是上面代码里的self.dataset变量.
** 调用数据集

比较简单,基本语法如下面第二行展示结果.

   #+BEGIN_SRC python
     test_dataset=NLG_GPT2_dataset(tokenizer=a_tokenizer, mode='test')
     test_loader=DataLoader(dataset=test_dataset, batch_size=args.batch_size, shuffle=True)
   #+END_SRC

DataLoader 来自于torch.utils.data.Dataloader

参数解释:
+ batch_szie: 规定batchsize
+ shuffle: 规定是否进行shuffle
+ dataset: 输入dataset

* 对于NLP使用过程中的特殊说明
NLP中, 使用huggingface transformers的话, 基本的流程(尤其对于中文)是这样的:
1. 将自然语言文本分成token, 得到token list;
2. 对token list 里的每一个元素, 都查询其在vocabulary list中的索引, 得到index tensor;
3. 将index tensor输入到模型里.

上述整个过程在上面的数据集封装中, 上述方法被self.tokenizer.single_convert_text_into_indextokens_and_segment_id()所实现.

下面给出自己对已有接口的一个自定义化的工程上的封装示例.


#+BEGIN_SRC python
  class MyTokenizer():
      def __init__(self, max_sentence_length=64):
	  path=os.path.dirname(os.path.abspath(__file__))
	  self.max_sentence_length=max_sentence_length
	  self.tokenizer=BertTokenizer.from_pretrained(path+'/Novel_GPT')
	  self.tokenizer.add_special_tokens({"additional_special_tokens":["[DOMAIN]","[NAME]",
	       "[NAME_ATBTE]",
	       "[AT_VALUE]",
	       "[POSITIVE]",
	       "[NEGATIVE]",
	       "[MAYBE_YOU_LIKE]",
	       "[DOYOU_LIKE_IT?]",
	       "[DOYOU_LIKE_IT?]"]})
      def __len__(self):
	  return len(self.tokenizer)
      def single_convert_text_into_indextokens_and_segment_id(self,text):
	  tokeniz_text = self.tokenizer.tokenize(text)
	  indextokens = self.tokenizer.convert_tokens_to_ids(tokeniz_text)
	  # indextokens.append(self.tokenizer.convert_tokens_to_ids('[EndOfResponse]'))
	  input_mask = [1] * len(indextokens)

	  if self.max_sentence_length<len(indextokens):
	      indextokens=indextokens[:self.max_sentence_length]
	      segment_id=[0]*self.max_sentence_length
	      input_mask=input_mask[:self.max_sentence_length]
	  else:
	      pad_indextokens = [0]*(self.max_sentence_length-len(indextokens))
	      indextokens.extend(pad_indextokens)
	      input_mask_pad = [0]*(self.max_sentence_length-len(input_mask))
	      input_mask.extend(input_mask_pad)
	      segment_id = [0]*self.max_sentence_length

	  indextokens=torch.tensor(indextokens,dtype=torch.long)
	  segment_id=torch.tensor(segment_id,dtype=torch.long)
	  input_mask=torch.tensor(input_mask,dtype=torch.long)

	  return indextokens,segment_id,input_mask

      def single_convert_text_into_tokeniz_textes(self,text):
	  tokenize_text=self.tokenizer.tokenize(text)
	  return tokenize_text

      def convert_text_into_indextokens_and_segment_id(self,text1,text2,spl='[SEP]'):
	  tokeniz_text1 = self.tokenizer.tokenize(text1)
	  indextokens1 = self.tokenizer.convert_tokens_to_ids(tokeniz_text1)

	  length_first=self.max_sentence_length//2
	  input_mask1 = [1] * len(indextokens1)

	  if length_first<len(indextokens1):
	      indextokens1=indextokens1[:length_first]
	      input_mask1=input_mask1[:length_first]
	  else:
	      pad_indextokens1 = [0]*(length_first-len(indextokens1))
	      indextokens1.extend(pad_indextokens1)
	      input_mask_pad1 = [0]*(length_first-len(input_mask1))
	      input_mask1.extend(input_mask_pad1)

	  text2=spl+text2
	  tokeniz_text2 = self.tokenizer.tokenize(text2)
	  indextokens2 = self.tokenizer.convert_tokens_to_ids(tokeniz_text2)

	  length_second=self.max_sentence_length-length_first
	  length2_begin=length_first
        
	  input_mask2 = [1] * len(indextokens2)

	  if length_second<len(indextokens2):
	      indextokens2=indextokens2[:length_second]
	      input_mask2=input_mask2[:length_second]
	  else:
	      pad_indextokens2 = [0]*(length_second-len(indextokens2))
	      indextokens2.extend(pad_indextokens2)
	      input_mask_pad2 = [0]*(length_second-len(input_mask2))
	      input_mask2.extend(input_mask_pad2)

            
	  indextokens1.extend(indextokens2)
	  input_mask1.extend(input_mask2)
	  indextokens=torch.tensor(indextokens1,dtype=torch.long)
	  # segment_id=torch.tensor(segment_id,dtype=torch.long)
	  input_mask=torch.tensor(input_mask1,dtype=torch.long)

	  return indextokens,None,input_mask

      def convert_ids_to_tokens(self, index):
	  return self.tokenizer.convert_ids_to_tokens(index)
      def convert_prediction_result2_sentence(self,output_prediction):
	  output_index=output_prediction.cpu()[0].numpy().tolist()
	  # print(output_index)
	  return self.tokenizer.convert_ids_to_tokens(output_index)
        
      def indextoken2wordtoken(self,index):
	  return self.tokenizer.convert_ids_to_tokens(index)

      def convert_tokens_to_ids(self,token):
	  return self.tokenizer.convert_tokens_to_ids(token)
      def get_max_seq_length(self):
	  return self.max_sentence_length
#+END_SRC


