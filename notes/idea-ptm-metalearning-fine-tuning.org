#+title: idea-预训练模型在小样本学习下的可解释性
#+author:liangzid 
#+FILETAGS: noshow, 
#+date: Fri Sep 10 11:32:30 2021
#+email: 2273067585@qq.com 

Why fine-tuning and prompt works? A meta-learning view.

* 相关工作的介绍

传统机器学习与元学习的区别：传统机器学习以样本为训练集、测试集的基本单位，而元学习以task为基本单位；
传统机器学习旨在学习一个较好的表示学习器或分类器，而元学习旨在学习一个较好的、在小样本下可以简单快速生成表示学习器的神经网络。

元学习包括三种思路：
1. 更好的模型初始化参数；
2. 更好的模型结构；
3. 更好的优化器

* 本论文的思路

Q：fine-tuning的思路意味着什么？

A1：预训练任务可以让预训练模型学到一个不错的超平面，在这个超平面上，不同的NLP的task都具有明显的区分，同时预训练之后的结果使得最终的模型初始参数恰恰落在这个超平面上；

A2：连接预训练模型的分类器，本质上相当于是在上述超平面上的一个点，如果该点距离各个task的平均距离比较近，则他们可以获得更好的效果；

Q：基于prompt的方法

基于prompt的方法主要包括两类，一种是设计输入pattern的方法，主要用在NLU上；还有一种是加prefix的方法，主要用在NLG模型上。
针对于这两种方法，可以看出：设计输入pattern的方法，其核心想法是在不改变超平面位置的前提下，拉近各个task与超平面中模型的距离；（这个任务与传统的meta-learning是相对的，不过相同的是二者的距离都被缩小了）；
而基于prefix的方法，其所起到的作用跟基于pattern的方法是一致的。但不同的是，基于pattern的方法提供的是一种离散的空间分布（因为输入只能是一些有意义的token），而prefix的方法则是使用一种连续的token输入，因而移动起来更容易一些，也更简单。

Q：对比学习的预训练任务意味着什么？

不同于MLM这种生成式任务，对比学习的预训练任务是基于相似性的，因此，基于对比学习的方法，更像是一种基于比较而进行meta-learning的方式。这种meta-learning的方式，可能更多的是想把一个task内部的诸多类别给区分开？？？