#+title: Introduction to the LLM librarys implemented in Rust: PART I: souce code analysis
#+date: Thu Nov  2 20:15:29 2023
#+author: Zi Liang
#+email: liangzid@stu.xjtu.edu.cn
#+latex_class: elegantpaper
#+filetags: ::


Recently there have seen increasing concerns about the effective
implementations of large-langauge models (LLMs). Beginning from [[https://github.com/ggerganov/llama.cpp][llama.cpp]],
there are so much works attempt to re-implement Transfomrer and Language
Models not with PYTHON, but another much more effective language, to support
the industrical application of LLMs.

Here I have read three classical librarys written in =rust=, and want
to introduce these three works. Specifically, this article only focused
on the implementations of these packages.

* candle 

[[https://github.com/huggingface/candle][candle]] by huggingface inc. is a quite hot library to do such things.
You cannot regard candle as a pure LLMs-based library, since it also
supports some other models like YOLO in computer vision. However, it
sure is due to LLMs that it comes so popular.

Here is an structure list provided by candle offical:

+ candle-core: Core ops, devices, and Tensor struct definition
+ candle-nn: Tools to build real models
+ candle-examples: Examples of using the library in realistic settings
+ candle-kernels: CUDA custom kernels
+ candle-datasets: Datasets and data loaders.
+ candle-transformers: transformers-related utilities.
+ candle-flash-attn: Flash attention v2 layer.

  I'd like to introduce this library in these aspects.


** candle-kernels: CUDA ops related.

dependency:

+ anyhow: this library provides anyhow::Error, a trait object based error type for easy idiomatic error handling in Rust applications.
+ glob: Support for matching file paths against Unix shell style patterns. example: \*.jpg
+ *rayoon*: data parallelism.


I cannot read clearly for this repository now.

** candle-core: tensor, device, and operations

dependencies:

+ accelerate-src& intel-mkl-src &
+ byteorder: encode or decode a number from big(little)-endian.
+ candle-kernels
+ cudar: rust binding of cuda
+ gemm: Playground for testing high performance matrix multiplication
+ half: f16 supports
+ libc: bindings to libc
+ memmap2: memory map. Map a =File= to =&[u8]= or =&mut [u8]=.
+ num-traits: Numeric traits for generic mathematics
+ num_cpus: obtain the cpu number of the machine.
+ rand: random related.
+ rand_distr: random distribution related.
+ rayon: parallelism related.
+ [[https://github.com/huggingface/safetensors][safetensors]]: another package by /huggingface/, to handle the _safe pytyorch tensor save_.
+ thiserror: a *derive* method to =std::error::Error=.
+ yoke: this crate provides Yoke<Y, C>, which allows one to “yoke” (attach) a zero-copy deserialized object (say, a Cow<'a, str>) to the source it was deserialized from, (say, an Rc<[u8]>), known in this crate as a “cart”, producing a type that looks like Yoke<Cow<'static, str>, Rc<[u8]>> and can be moved around with impunity.
+ zip: handle zip files.



Core functions similar to =torch=.

it consists of following types/traits:

1. CpuStorage(ENUM): the sotrage form of all data in CPU. Supports: u8,u32,i64,f16,f32,f64 now.
2. Device: virtual device symbol
3. DeviceLocation: similar to torch.deivce
4. DType, FloatDType, IntDType, WithDType: data type and element-related operations.
5. Error, Result: error and result wrapper for candle;
6. IndexOp: to use slice like a[:-1,0,:128]
7. shape::{Shape, D}: Shape is a vector of shape; D: I don't know. Maybe it is  hack to match two tensors for operations like multiplication.
8. Layout: to show the storage of a tensor, is contiguous in C style on hardware or not.
9. CustomOp1, CustomOp2, CustomOp3: the trait to implement a unary, binary, or triple opertions by users. It defines the basic method a operation should have: forward() in CPU and GPU, and backward().
10. storage::Storage: the union of CupStorage and CudaStorage.
11. strided_index::{StridedBlocks, StridedIndex}: see =layout=
12. tensor::{Tensor, TensorId}:Tensor is similar to torch.tensor. TensorId: the point of a tensor.
13. variable::Var: Similar to =Variable= in old torch? =Var= can be changed while =Tensor= not.
14. Module: a trait for who need to support the forward computation.












** candle-nn:
dependencies not occred before:
+ serde: handle json format, like json in python.


it consists of following things:
+ activation::Activation;
+ batch_norm::{batch_norm, BatchNorm, BatchNormConfig};
+ conv:
+ embedding::{embedding, Embedding};
+ func::{func, func_t, Func, FuncT};
+ group_norm::{group_norm, GroupNorm};
+ init::Init;
+ layer_norm::{layer_norm, rms_norm, LayerNorm, LayerNormConfig, RmsNorm};
+ linear::{linear, linear_no_bias, Linear};
+ ops::Dropout;
+ optim::{AdamW, Optimizer, ParamsAdamW, SGD};
+ rnn::{gru, lstm, GRUConfig, LSTMConfig, GRU, LSTM, RNN};
+ sequential::{seq, Sequential};
+ var_builder::VarBuilder;
+ var_map::VarMap;
+ candle::{Module, ModuleT};
