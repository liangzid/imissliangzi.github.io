#+title: Language Models as Knowledge Bases? 
#+OPTIONS: html-style:nil
#+date: Thu Nov 24 16:42:04 2022
#+author: Zi Liang
#+email: liangzid@stu.xjtu.edu.cn
#+latex_class: elegantpaper
#+filetags: ::


* 相关benchmark及对应效果
  LAMA


* 相关工作

** Language Models as Knowledge Bases? ‘19

 出发点：构建并使用KB需要一个完整的麻烦的pipeline，如果LM在语法知识之外还可以学会文档中的关系型知识，那么就不需要如此麻烦。该论文试图探究：LM预训练模型中是否含有关系型知识？和已有的方法比效果如何？

[[file:./images/screenshot_20221124_171406.png]]   


结论：
1. 即使不做微调，LM里面也含有关系型的知识
2. BERT通过完形填空任务做QA，效果还是挺好的


该文属于开山之作，提出了这种可能

** Neural Databases '20

   出发点：LM能否作为数据库来用，即我们通过自然语言进行数据库的一系列操作，诸如查询、更新、删除等等。

   作者首先测试了当前的LM的效果，发现：1）在slect-project-join querys 上够用，如果已经拥有了对应的事实；2）在一些特殊操作（no-trival databases）或聚合操作的查询上，不太够用。

   为此作者自己提出了一套architecture（NeuralDB）解决这个问题。输入大概是这个样子：

  [[file:./images/screenshot_20221124_172725.png]] 

  可以看出，这其实也就是非结构化的知识信息。

  模型的推理时刻表示图，backbone是t5

 [[file:./images/screenshot_20221124_172829.png]] 

作者统计了在各种数据库查询问题上的结果，如下：

[[file:./images/screenshot_20221124_173016.png]] 


于是作者自己搞了一个模型（NeuralDB），总体上看，就是加了一些trick，效果图如下：

[[file:./images/screenshot_20221124_173154.png]]

具体流程就是，通过检索获得一些支撑集合。然后动过比对这些支撑集合来生成答案。最终通过聚合器聚合这些答案，得到最终的结果。

之前写过一篇笔记，见：https://liangzid.github.io/paper_reading/neural_database.html


** Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View ACL'22









   
