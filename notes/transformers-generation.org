#+title: transformers源码分析之 生成函数（generation）
#+OPTIONS: html-style:nil
#+date: Mon Oct  3 16:38:42 2022
#+author: Zi Liang
#+email: liangzid@stu.xjtu.edu.cn
#+latex_class: elegantpaper
#+filetags: ::


transformers做文本生成的基本思路是：封装一个包含了生成的所有流程的 =MixMin= 类，并默认让所有生成模型都继承这个类。

在这个MixIn类中， =generate= 方法是总入口，使得用户可以通过调用generate来调用整个函数。

一般而言，文本生成被认为存在四种结构，这四种结构分别由两类组成：模型是贪婪采样（即argmax）还是按照分布概率采样;模型是greedy search还是beam search。在进行NLG的约束时，主要的约束包括：
1. logitsProcessor
2. LogitsWarper
二者的输入与输出格式差距不大。
1. 输入： 一个shape 为 （bs, msl）的input index;一个shape为（bs, vocab_len）的对nextoken的一个预测分布
2. 输出：一个新的分布

Processor和Warper都旨在通过一些操作来对上述这个输入的distribution进行扭曲变动，最终返回一个新的分布。其实这二者没有太大的区别，唯一的区别可能是warper更多是一些连续的变形操作，而processor中常常样会有自动赋值等做法。




* 










