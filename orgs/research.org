#+title: Zi Liang (Research Page)
#+OPTIONS: html-style:nil
#+author:liangzid 
#+FILETAGS: noshow, 
#+date: Fri Sep 10 14:23:46 2021
#+email: 2273067585@qq.com 

[[file:images/danjin.jpg]]

* Introduction

** Introducing Myself

My name is Zi Liang, now a PhD student in the [[https://www.astaple.com/][Astaple Group]] of Hong Kong Polytechnic University (PolyU). My supervisor is Prof. [[https://haibohu.org/][Haibo Hu]]. I begin my research in Xi'an Jiaotong University, under the supervision of Prof. [[https://gr.xjtu.edu.cn/web/phwang][Pinghui Wang]] and [[https://www.linkedin.com/in/ruofei][Ruofei (Bruce) Zhang]].
I also work closely with Dr. [[https://scholar.google.com.hk/citations?user=XzO2dV0AAAAJ&hl=zh-CN][Nuo Xu]], Dr. [[https://scholar.google.com.hk/citations?user=Wd5IdkMAAAAJ&hl=zh-TW][Shuo Zhang]], [[https://scholar.google.com/citations?user=spRkQ2oAAAAJ&hl=en][Yaxin Xiao]], [[https://xinweizhang1998.github.io/xinweizhang.github.io/][Xinwei Zhang]], [[https://sites.google.com/view/liujunxu][Junxu Liu]] and [[https://yywang.netlify.app/][Yanyun Wang]].


I focus on analyzing (pursue) the potential risks contained in current transformer-based large language models and dive into the analysis of why and how neural networks work and raise a vulnerability.
My research can be divided into the following two categories:

+ *Revealing new threats or defence against existing attacks*. To provide a comprehensive evaluation on popular AI services or techniques incorporated with in-depth theoretical analysis or a series of intuitive explorations, such as my recent study in /prompt extraction attacks/ ([[https://arxiv.org/abs/2408.02416][link]]);
+ *Better understanding to models and learning*. To explain why and how safety issues produced and what such a problem means during the training/inference of models.

Also, I am very familiar with /natural language processing/ since 2020, especially in constructing conversational AIs. From 2024 I am also excited to the future of AI when employing /reinforcement learning/.

** Introducing My Recent Research

*** Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models [Arxiv'24]

This paper uncover the threat of *prompt leakage* on customized prompt-based services, such as OpenAI's GPTs. It aims to answer three questions:
1. Can LLM's alignments defend against prompt extraction attacks?
2. How do LLMs leak their prompts?
3. Which factors of prompts and LLMs lead to such leakage?


We provide a comprehensive and systemic evaluation to answer question 1 and 3, and propose two hypothesis with experimental validation for question 2. We also propose several easy-to-adpot defending strategy based on our discovery.

Click [[https://arxiv.org/abs/2408.02416][here]] if you are also interested in this research.

*** MERGE: Fast Private Text Generation [AAAI'24]

This paper propose a new privacy-preserving inference framework for current transformer-based generative language models based on Secret Sharing and Multi-party Security Computation (MPC). It is also the *first* private inference framework specifically designed for NLG models. 10x of speedup are provided via our propose method.

If you are curious about how cryptography protect the privacy of user contents and models and how we optimize the inference procedure, click [[https://ojs.aaai.org/index.php/AAAI/article/view/29964][here]] for more details.

* Experiences
1. 2016.09-2020.06: Bachelor Degree, in Northeastern University, on /cybernetics (Control Theory)/;
2. 2020.09-2023.06: Master Degree, in the iMiss Group of Xi'an Jiaotong University, on /software engineer/ and research for /Conversational AI/ and /NLP Security/;
3. 2023.11-now: PhD Student, in the The Hong Kong Polytechnic University in Hong Kong. Research of interests: /AI security/ and /Natural Language Processing/.
* Publications 
+ /PAIR: Pre-denosing Augmented Image Retrieval Model for Defending Adversarial Patches/ Z Zhou, P Wang, *Z Liang*, R Zhang, H Bai - MM 2024
+  /Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models/ *Z Liang*, H Hu, Q Ye, Y Xiao, H Li - arXiv preprint arXiv:2408.02416, 2024 [[[https://arxiv.org/abs/2408.02416][Paper]]][[[https://github.com/liangzid/PromptExtractionEval][Code]]]
+ /TSFool: Crafting Highly-Imperceptible Adversarial Time Series through Multi-Objective Attack/ Yanyun Wang, Dehui Du, Haibo Hu,  *Zi Liang*, Yuanhao Liu - ECAI, 2024
+ /Merge: Fast private text generation/  *Z Liang*, P Wang, R Zhang, N Xu, S Zhang, L Xing… - AAAI, 2024 [[[https://arxiv.org/abs/2305.15769][Paper]]] [[[https://github.com/liangzid/MERGE][Code]]] 
+ "Healing Unsafe Dialogue Responses with Weak Supervision Signals." *Z Liang*, ... - arXiv preprint arXiv:2305.15757 (2023). [[[https://arxiv.org/abs/2305.15757][Paper]]][[[https://github.com/liangzid/TEMP][Code]]]
+ /Multi-action dialog policy learning from logged user feedback/ S Zhang, J Zhao, P Wang, T Wang,  *Z Liang*, J Tao… - AAAI, 2023
* Contact Me 
**** GitHub: https://github.com/liangzid
**** MAIL: zi1415926.liang@connect.polyu.hk 
**** Wechat: frostliangzi
**** Google Scholar: [[https://scholar.google.com/citations?user=pzrGwvMAAAAJ&hl=zh-CN][HERE]]
